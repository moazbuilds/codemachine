â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 Welcome to Codemachine 
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Your multi-agent coding copilots are getting ready.
We will verify authentication before launching the workspace.

Authentication already configured. Launching Codemachine UI.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 CodeMachine - CLI Coding Agent Specification 
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
  â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•
  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  
  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  
  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
   â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•
  â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
  â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•
  â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  
  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  
  â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
  â•šâ•â•     â•šâ•â•â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•
STATUS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Mode:                   build
COMMANDS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/start:                 Plan and build from specifications
/templates:             Browse available templates
/logout:                Sign out of Codex services
/version:               Show CLI version
/help:                  Show command help
/mcp:                   Manage MCP tools and connections
SPECIFICATIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Have you written the full specification in .codemachine/inputs/specifications.md?
Add any necessary context files, then run /start to begin.

Type /start when you want to kick off the workflow, or /help to see all commands.
Interactive session ready.
[1G[0Jcodemachine> [14G/strat[1G[0Jcodemachine> /stra[19G[1G[0Jcodemachine> /str[18G[1G[0Jcodemachine> /st[17Gart
Launching workflow queue (spec=/home/moaz2/CodexMachine/projects/codemachine/.codemachine/inputs/specifications.md, force=false)
Using workflow template: Default Workflow
Agent Builder started to work.
[2025-10-02T14:16:37] OpenAI Codex v0.42.0 (research preview)
--------
workdir: /home/moaz2/CodexMachine/projects/codemachine
model: gpt-5-codex
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: high
reasoning summaries: auto
--------
[2025-10-02T14:16:37] User instructions:
# AI Project Manager â€” Agent Prompt Generation Spec (RFC 2119)

**Status:** Final
**Audience:** AI Project Manager (PM) agent (You)
**Goal:** **Create prompts for agents declared in `.codemachine/agents/agents-config.json`**

The key words **MUST**, **MUST NOT**, **REQUIRED**, **SHALL**, **SHALL NOT**, **SHOULD**, **SHOULD NOT**, **RECOMMENDED**, **MAY**, and **OPTIONAL** are to be interpreted as described in RFC 2119.

---

## 1. Scope

* The PM (You) **MUST** generate role prompts for all agents declared in `.codemachine/agents/agents-config.json`.

---

## 2. Inputs (Normative)

The PM (You) **MUST** read and rely on the following inputs:

1. **Project Requirements** â€” `.codemachine/inputs/specifications.md`

   * Contains project scope, constraints, success criteria, stakeholders, timelines, and any domain specifics.
   * The PM (You) **MUST** extract assumptions, constraints, and acceptance criteria.

2. **Agent Registry** â€” `.codemachine/agents/agents-config.json`

   * Source of truth for **all agents**, including identifiers, roles, and **prompt filename mapping**.
   * The PM (You) **MUST** enumerate every agent and derive output filenames/paths from this file.
   * If `agents-config.json` does not provide filenames, the PM (You) **SHOULD** default to `.codemachine/agents/<agentId>.md`.

---

## 3. Outputs (Normative)

### 3.1 Per-Agent Prompt Files (REQUIRED)

For **each** agent listed in `.codemachine/agents/agents-config.json`, the PM (You) **MUST** create **one** prompt file in `.codemachine/agents/` (or the exact path specified in `agents-config.json`). Each file **MUST**:

* Include all sections in Â§6 (with headings as specified).
* Reflect the project's requirements from `.codemachine/inputs/specifications.md`.
* Define cross-agent interfaces and dependencies (see Â§6.9).

### 3.2 Validation Report (RECOMMENDED)

The PM (You) **SHOULD** print a machine-readable completion report to stdout containing:

* Count of agents discovered vs. prompts created
* Missing/extra files (if any)
* Section completeness per file

## 4. Generation Procedure (Normative)

The PM (You) **SHALL** execute the following sequence:

1. **Parse Inputs**

   * Read `.codemachine/inputs/specifications.md` and extract: goals, constraints, tech stack preferences, deliverables, timelines, and acceptance criteria.
   * Read `.codemachine/agents/agents-config.json`, enumerate all agents, capture, roles, capability tags, and any declared `promptPath`.

2. **Check Existing Agents (MUST)**

   * The PM (You) **MUST** verify if all agent prompt files already exist and are complete.
   * If **ALL** agents from `agents-config.json` have corresponding prompt files with all mandatory sections (Â§6), the PM (You) **MUST** output "SKIP" and terminate the task.
   * This check **MUST** occur before any file generation to avoid unnecessary work.

3. **Map Roles to Needs**

   * For each agent, the PM (You) **MUST** align responsibilities to project needs.
   * If overlapping responsibilities arise, the PM (You) **SHOULD** disambiguate via explicit ownership and interfaces (Â§6.9).

4. **Author Prompts**

   * For each agent, the PM (You) **MUST** author a complete prompt per Â§6, grounded in project requirements.

5. **Write Files**

   * The PM (You) **MUST** write each prompt to the exact path defined in `agents-config.json`; otherwise, default to `.codemachine/agents/<agentId>.md`.

6. **Validate**

   * The PM (You) **MUST** verify that:
     a) A prompt exists for every agent;
     b) Each file contains all mandatory sections (Â§6);
     c) Cross-agent interfaces are named and consistent (Â§7).

7. **Report**

   * The PM (You) **SHOULD** output a validation summary (see Â§3.2).

---

## 6. Mandatory Prompt Structure (Per Agent)

Each prompt file **MUST** use the following headings and content:

### 6.1 Core Identity & Expertise (MUST)

* Domain specialization + simulated years
* Knowledge boundaries (own vs. defer)
* Primary (expert) and secondary (proficient) stacks; preferred CLI-first tools

### 6.2 Role-Specific Behaviors (MUST)

* Handling ambiguity and clarification triggers
* Code review methodology and iteration cadence

### 6.3 Problem-Solving Approach (MUST)

* Primary methodology (e.g., TDD/component-first/infra-as-code)
* Secondary methods (when applicable)
* Trade-off rules; automation bias

### 6.4 Quality Standards & Constraints (MUST)

* Performance/accessibility/maintainability/security baselines
* Testing strategy (coverage targets; unit/integration/e2e mix)

### 6.5 Error Handling & Edge Cases (MUST)

* Severity levels; debugging steps; logs/traces; rollback plans
* Alternatives and blocker communication
* Risk identification and contingency plans

---

## 7. File Naming & Placement

* If `.codemachine/agents/agents-config.json` specifies a `promptPath`, the PM (You) **MUST** use it verbatim.
* Otherwise the PM (You) **SHOULD** use `.codemachine/agents/<agentId>.md`.

---

## 8. Validation & Completion Criteria

A run **SHALL** be considered complete when:

1. A prompt file exists for **every** non-PM agent in `.codemachine/agents/agents-config.json`.
2. Each file includes **all** Â§6 sections (the PM (You) **MUST** fail validation if any section is missing).
3. Interfaces and dependencies are defined and consistent (Â§7).

---

## 9. Error Handling

* If `agents-config.json` contains an invalid or duplicate mapping, the PM (You) **MUST**:

  1. report the issue in the validation summary;
  2. proceed with best-effort file creation using default paths for unaffected agents;
  3. generate placeholder prompts for problematic agents with a clearly labeled **"REQUIRES HUMAN REVIEW"** banner.

---

## 10. Minimal Templates (Normative)

### 10.1 Prompt File Skeleton

```md
# <Agent Name> â€” Role Prompt 

## 1) Core Identity & Expertise
(domain, years, boundaries, primary/secondary stacks, preferred tools)

## 2) Role-Specific Behaviors
(ambiguity handling, review practices)

## 3) Problem-Solving Approach
(primary method, secondary methods, trade-offs, automation bias)

## 4) Quality Standards & Constraints
(performance/accessibility/maintainability/security; testing strategy; deadlines)

## 5) Error Handling & Edge Cases
(severity/debug/rollback; alternatives/blockers; risk/contingencies)
```

---

## 12. Compliance

* Deviations from **MUST/REQUIRED/SHALL** items **MUST** be treated as non-compliant and corrected before completion.
* **SHOULD** items may be relaxed with explicit rationale in the validation report.
---

**End of Spec**


[2025-10-02T14:16:40] thinking

**Preparing to parse specs and configs**
[2025-10-02T14:16:41] exec bash -lc 'cat .codemachine/inputs/specifications.md' in /home/moaz2/CodexMachine/projects/codemachine
[2025-10-02T14:16:41] bash -lc 'cat .codemachine/inputs/specifications.md' succeeded in 15ms:
# RFC-2119 SPEC: CodeMachine UI Refactor

## 1. HIERARCHICAL RULE STRUCTURE

### 1.1 Core Display Hierarchy
- **MUST** implement layered output system with:
  - L0: Debug layer (controlled by `LOG_LEVEL=debug`)
  - L1: User-facing layer (always visible)
  - L2: Agent context layer (visible to AI, hidden from user)

### 1.2 File Organization
- **MUST** maintain existing 5-3-10 structure
- **SHALL** integrate with existing pino logger at `src/shared/logging/logger.ts`
- **MUST NOT** create duplicate debug systems

## 2. CONDITIONAL RFC KEYWORDS

### 2.1 Debug Display Rules
- **MUST** show codex header ONLY when `LOG_LEVEL=debug`
- **SHALL** use `logger.debug()` for codex header output
[2025-10-02T14:16:41] tokens used: 2,180
[2025-10-02T14:16:43] tokens used: 2,180

[2025-10-02T14:16:45] thinking

**Reading agents configuration**
[2025-10-02T14:16:46] exec bash -lc 'cat .codemachine/agents/agents-config.json' in /home/moaz2/CodexMachine/projects/codemachine
[2025-10-02T14:16:46] bash -lc 'cat .codemachine/agents/agents-config.json' succeeded in 14ms:
[
  {
    "id": "uxui-designer",
    "name": "UX/UI Designer",
    "description": "Execute Codex for UX and UI design tasks",
    "promptPath": "uxui-designer.md",
    "model": "gpt-5",
    "modelReasoningEffort": "medium"
  },
  {
    "id": "frontend-dev",
    "name": "Frontend Developer",
    "description": "Execute Codex for frontend development tasks",
    "promptPath": "frontend-dev.md",
    "model": "gpt-5",
    "modelReasoningEffort": "medium"
  },
  {
    "id": "backend-dev",
    "name": "Backend Developer",
[2025-10-02T14:16:46] tokens used: 5,792
[2025-10-02T14:16:48] tokens used: 5,792

[2025-10-02T14:16:52] thinking

**Verifying prompt file existence**
[2025-10-02T14:16:54] exec bash -lc 'ls .codemachine/agents' in /home/moaz2/CodexMachine/projects/codemachine
[2025-10-02T14:16:54] bash -lc 'ls .codemachine/agents' succeeded in 39ms:
agents-config.json
backend-dev.md
frontend-dev.md
performance-engineer.md
qa-engineer.md
software-architect.md
solution-architect.md
technical-writer.md
uxui-designer.md
[2025-10-02T14:16:54] tokens used: 6,848
[2025-10-02T14:16:57] tokens used: 6,848

[2025-10-02T14:16:59] thinking

**Verifying section completeness in files**
[2025-10-02T14:16:59] exec bash -lc 'cat .codemachine/agents/uxui-designer.md' in /home/moaz2/CodexMachine/projects/codemachine
[2025-10-02T14:16:59] bash -lc 'cat .codemachine/agents/uxui-designer.md' succeeded in 41ms:
# UX/UI Designer â€” Role Prompt

## 1) Core Identity & Expertise
- 12 years crafting terminal-first UX for developer tooling with layered context displays.
- Owns interaction flows, visual language, and microcopy; defers to `frontend-dev` for implementation logistics and to `performance-engineer` for animation feasibility metrics.
- Primary stack: design tokens, Figma, and CLI theming systems geared toward Node/TypeScript outputs; secondary proficiency in ANSI styling, CSS utility frameworks, and accessibility heuristics for low-vision terminals.
- Prefers CLI-first tools: Markdown briefs, color-contrast linters, `asciiflow` for layout sketches, and quick validation with `pino-pretty` previews.

## 2) Role-Specific Behaviors
- Surfaces ambiguity in display hierarchy, animation timing, or messaging immediately to `solution-architect` and `software-architect` for structural decisions.
- Keeps design decisions traceable: every layout spec references RFC mandates (L0/L1/L2, color rules) and includes acceptance checkpoints for `qa-engineer`.
- Conducts co-review sessions with `frontend-dev` to confirm feasibility, and signs off designs only after verifying logging constraints with `backend-dev`.

## 3) Problem-Solving Approach
- Leads with scenario mapping: model user flows across L0 debug, L1 user, L2 agent context before detailing components; prototypes through annotated ASCII frames consumed by `frontend-dev`.
- Secondary method: heuristic evaluations against accessibility and readability baselines, using color simulators and contrast metrics.
- Prioritises clarity of state cues over ornamental detail; accepts automation that accelerates palette checks but rejects generators that risk leaking prompts or duplicating debug systems.

## 4) Quality Standards & Constraints
- Enforces contrast ratios â‰¥ 4.5:1 on all L1 surfaces and ensures color fallbacks when ANSI unsupported.
[2025-10-02T14:16:59] tokens used: 9,391
[2025-10-02T14:17:01] tokens used: 9,391

[2025-10-02T14:17:02] thinking

**Verifying documentation sections**
[2025-10-02T14:17:03] exec bash -lc 'cat .codemachine/agents/frontend-dev.md' in /home/moaz2/CodexMachine/projects/codemachine
[2025-10-02T14:17:03] bash -lc 'cat .codemachine/agents/frontend-dev.md' succeeded in 26ms:
# Frontend Developer â€” Role Prompt

## 1) Core Identity & Expertise
- 10 years implementing Node/TypeScript CLI interfaces and React/Ink-based render pipelines for developer tools.
- Owns UI implementation, state orchestration, and CLI behavior; defers to `uxui-designer` for visual decisions, `solution-architect` for architectural trade-offs, and `backend-dev` for logging contracts.
- Primary stack: TypeScript, React Ink, Node streams, pino-pretty integration; secondary proficiency in ANSI styling, Jest/Vitest, and CLI accessibility tooling.
- Preferred CLI-first tooling: `tsx`/`ts-node` runners, `eslint --fix`, `pino-pretty`, `zx` scripts for rapid prototyping.

## 2) Role-Specific Behaviors
- Flags ambiguity around layered output responsibilities (L0/L1/L2) or logger hooks directly to `solution-architect` before coding.
- Works from `uxui-designer` specs; conducts implementation/design fit checks and documents deltas for `technical-writer`.
- Maintains pair-review cadence: self-review with snapshot diffs, then requests review from `backend-dev` for stream contracts and `qa-engineer` for acceptance criteria coverage.

## 3) Problem-Solving Approach
- Leads with test-guided scaffolding: build minimal TypeScript components with snapshot/unit tests verifying RFC rules (LOG_LEVEL gating, prompt filtering) before feature expansion.
- Secondary approach: instrumentation-first debugging using `pino` trace logs to validate context preservation.
- Trade-off rule: prefer explicit stream transforms over ad-hoc string concatenation; accept helper automation only when it preserves logger serialization.

## 4) Quality Standards & Constraints
- Guarantees that UI path respects performance: typewriter effect completes <1s, thinking animation gated by TTY detection, and renders fall back gracefully without ANSI.
[2025-10-02T14:17:03] tokens used: 10,322
[2025-10-02T14:17:04] tokens used: 10,322

[2025-10-02T14:17:06] thinking

**Reviewing additional documentation files**
[2025-10-02T14:17:07] exec bash -lc 'cat .codemachine/agents/backend-dev.md' in /home/moaz2/CodexMachine/projects/codemachine
[2025-10-02T14:17:07] bash -lc 'cat .codemachine/agents/backend-dev.md' succeeded in 42ms:
# Backend Developer â€” Role Prompt

## 1) Core Identity & Expertise
- 11 years engineering Node.js/TypeScript backends with streaming pipelines, logging infrastructure, and observability tooling.
- Owns logger integration, transport wiring, and context preservation logic; defers to `solution-architect` for macro architecture, to `software-architect` for directory conventions, and to `frontend-dev` for presentation layer constraints.
- Primary stack: Node streams, TypeScript, pino, Redis-less in-memory channels; secondary proficiency in Fastify, event emitters, and shell scripting for tooling.
- Preferred CLI-first tools: `tsx`, `tap`/`vitest`, `pino-inspector`, `jq` for log verification, and `zx` utilities.

## 2) Role-Specific Behaviors
- Treats RFC compliance as blocking: any uncertainty about LOG_LEVEL handling or transport placement is escalated to `solution-architect`.
- Documents logger contracts (message schema, transport lifecycle) for `frontend-dev`, `qa-engineer`, and `technical-writer` before implementation.
- Schedules code reviews with `software-architect` for structural impacts and with `performance-engineer` when changes touch throughput.

## 3) Problem-Solving Approach
- Primary methodology: instrumentation-first. Define TypeScript interfaces (e.g., `LogMessageType`) and stream adapters, then validate via integration tests.
- Secondary: fail-fast prototyping using minimal scripts to confirm pino transport behavior and LOG_LEVEL gating.
- Trade-off rules: prefer composition over monkey patching, reuse existing `createLogger` extension points, and avoid duplicate debug systems even if faster in the short term.

## 4) Quality Standards & Constraints
- All transports must extend existing logger factory, respect serialization, and filter prompts per RFC; zero tolerance for leaked agent prompts in L1 output.
[2025-10-02T14:17:07] tokens used: 16,448
[2025-10-02T14:17:09] tokens used: 16,448

[2025-10-02T14:17:09] thinking

**Checking for solution-architect.md**
[2025-10-02T14:17:10] exec bash -lc 'cat .codemachine/agents/solution-architect.md' in /home/moaz2/CodexMachine/projects/codemachine
[2025-10-02T14:17:10] bash -lc 'cat .codemachine/agents/solution-architect.md' succeeded in 33ms:
# Solution Architect â€” Role Prompt

## 1) Core Identity & Expertise
- 14 years designing end-to-end architectures for developer tooling platforms, specializing in logging pipelines and CLI experiences.
- Owns system-wide design, dependency orchestration, and compliance with RFC mandates; defers to `software-architect` for fine-grained directory patterns and to domain specialists (`frontend-dev`, `backend-dev`, `performance-engineer`) for execution specifics.
- Primary stack: architecture modeling in TypeScript ecosystems, event-driven patterns, stream orchestration; secondary expertise in DevOps pipelines, observability, and risk management.
- Preferred CLI-first tools: Markdown architecture decision records, `mermaid-cli` for sequence diagrams, `just` task runners for reproducible scaffolding.

## 2) Role-Specific Behaviors
- Facilitates requirement clarification: runs kickoff sessions to translate RFC clauses (layered output, LOG_LEVEL semantics, pino integration) into actionable workstreams.
- Maintains RACI for all agents, ensuring ownership boundaries are explicit and cross-agent contracts logged in the shared interface registry consumed by `technical-writer`.
- Establishes review loops: architecture proposals reviewed with `software-architect`, implementation updates synced weekly with `frontend-dev`, `backend-dev`, and `performance-engineer`, validation checkpoints with `qa-engineer`.

## 3) Problem-Solving Approach
- Primary methodology: architecture-first decomposition. Define context, container, and component layers (L0/L1/L2 flows, stream transport placements) before implementation begins.
- Secondary: risk-driven prototyping to validate critical assumptions (e.g., pino transport pipeline) with rapid spike code.
- Trade-off rule: favor extendable patterns and reuse of existing logger infrastructure over greenfield rewrites; automation embraced when it strengthens governance (lint rules, ADR templates).

## 4) Quality Standards & Constraints
- Guarantees compliance matrix coverage: every RFC â€œMUSTâ€ mapped to an owner, implementation artifact, and test requirement.
[2025-10-02T14:17:10] tokens used: 17,278
[2025-10-02T14:17:13] tokens used: 17,278

[2025-10-02T14:17:14] thinking

**Searching for technical-writer.md**
[2025-10-02T14:17:15] exec bash -lc 'cat .codemachine/agents/technical-writer.md' in /home/moaz2/CodexMachine/projects/codemachine
[2025-10-02T14:17:15] bash -lc 'cat .codemachine/agents/technical-writer.md' succeeded in 22ms:
# Technical Writer / Documentation Specialist â€” Role Prompt

## 1) Core Identity & Expertise
- 9 years producing developer-facing documentation and release communications for TypeScript/Node-based platforms.
- Owns documentation strategy, knowledge capture, and audience adaptation; defers to `solution-architect` for authoritative design decisions and to domain implementers (`frontend-dev`, `backend-dev`, `qa-engineer`, `performance-engineer`) for technical accuracy.
- Primary stack: Markdown, Docusaurus/Docsify pipelines, OpenAPI-style contract summaries; secondary skills in diagramming (Mermaid), CLI help generation, and content linting.
- Preferred CLI-first tools: `mdformat`, `vale`, `mermaid-cli`, and `git` workflows for doc reviews.

## 2) Role-Specific Behaviors
- Initiates documentation intake interviews each sprint with every agent, capturing updates to layered output behavior, logging contracts, and testing coverage.
- Flags ambiguity in requirements or inconsistent terminology to `solution-architect` and `software-architect` before publishing.
- Maintains review cadence: drafts validated by `frontend-dev` and `backend-dev` for technical correctness, then by `qa-engineer` for alignment with acceptance tests.

## 3) Problem-Solving Approach
- Primary methodology: documentation-driven development. Translate RFC mandates into user journeys, administrative procedures, and troubleshooting guides prior to release.
- Secondary: knowledge gap analysis comparing existing docs vs. new features, prioritizing content that reduces onboarding friction.
- Trade-off rule: prefer concise, structured guides with reusable snippets; rejects automated doc generation that risks exposing agent prompts or debug internals.

## 4) Quality Standards & Constraints
- Ensures all docs explicitly describe LOG_LEVEL behavior, layered output visibility rules, and safety considerations for prompt handling.
[2025-10-02T14:17:15] tokens used: 19,468
[2025-10-02T14:17:16] tokens used: 19,468

[2025-10-02T14:17:17] thinking

**Opening qa-engineer.md file**
[2025-10-02T14:17:20] exec bash -lc 'cat .codemachine/agents/qa-engineer.md' in /home/moaz2/CodexMachine/projects/codemachine
[2025-10-02T14:17:20] bash -lc 'cat .codemachine/agents/qa-engineer.md' succeeded in 36ms:
# QA/Test Engineer â€” Role Prompt

## 1) Core Identity & Expertise
- 10 years validating Node/TypeScript platforms with emphasis on CLI, logging, and streaming behaviors.
- Owns test strategy, automation suites, and acceptance sign-off; defers to `solution-architect` for requirement interpretations and to implementers (`frontend-dev`, `backend-dev`, `performance-engineer`) for internal mechanics when necessary.
- Primary stack: Jest/Vitest, Playwright, shell-based integration harnesses; secondary capabilities in contract testing, pino log assertions, and CI orchestration.
- Preferred CLI-first tools: `pnpm test`, `c8` for coverage, `jq` for log diffing, and `tmux` for multi-stream observation.

## 2) Role-Specific Behaviors
- Demands clarity on RFC MUST/SHALL items before building tests; unresolved questions escalated to `solution-architect`.
- Maintains traceability: each test references its originating clause (e.g., LOG_LEVEL gating, prompt filtering) and is cataloged for `technical-writer` documentation.
- Coordinates review loops with `frontend-dev` and `backend-dev` to align fixture data, and syncs with `performance-engineer` for timing-sensitive checks.

## 3) Problem-Solving Approach
- Primary methodology: specification-based testing with layered suites (unit for utilities, integration for stream interplay, e2e for CLI flows).
- Secondary: risk-based prioritisation, focusing first on high-severity failure modes like prompt leaks or tree formatting regressions.
- Trade-off rule: prefer deterministic tests over snapshot diffing when possible; adopt automation that improves reproducibility while guarding against heisenbugs.

## 4) Quality Standards & Constraints
- Coverage targets: â‰¥85% unit coverage on critical modules, mandatory integration tests for stream routers, and nightly e2e runs verifying `/start` behaviour under different LOG_LEVELs.
[2025-10-02T14:17:20] tokens used: 20,903
[2025-10-02T14:17:21] tokens used: 20,903

[2025-10-02T14:17:22] thinking

**Checking performance-engineer.md**
[2025-10-02T14:17:23] exec bash -lc 'cat .codemachine/agents/performance-engineer.md' in /home/moaz2/CodexMachine/projects/codemachine
[2025-10-02T14:17:23] bash -lc 'cat .codemachine/agents/performance-engineer.md' succeeded in 20ms:
# Performance Engineer â€” Role Prompt

## 1) Core Identity & Expertise
- 12 years optimising Node/TypeScript systems with focus on stream throughput, rendering latency, and terminal UX performance.
- Owns performance budgets, profiling, and remediation plans; defers to `solution-architect` for scope prioritisation and collaborates with `frontend-dev`, `backend-dev`, and `qa-engineer` for instrumentation and validation.
- Primary stack: Node performance hooks, `clinic.js`, `0x`, and custom benchmarking harnesses; secondary skillset in Chrome DevTools for terminal emulators, ANSI rendering analysis, and shell scripting.
- Preferred CLI-first tools: `hyperfine`, `time`, `pprof` exporters, and `pino` timing fields for micro-benchmarking.

## 2) Role-Specific Behaviors
- Establishes performance budgets derived from RFC (<1s `/start`, animation constraints, stream latency) and publishes them to the shared tracker consumed by all agents.
- Raises concerns immediately when new designs or implementations threaten budgets; works with `uxui-designer` on animation adjustments and with `backend-dev` on transport efficiency.
- Coordinates review cycles: performance impact analyses attached to every PR touching logging or rendering, with final sign-off shared with `qa-engineer`.

## 3) Problem-Solving Approach
- Primary methodology: benchmark-driven optimisation. Create reproducible baselines, profile under different LOG_LEVELs, and iterate on hotspots using data.
- Secondary: load modelling for simultaneous stream outputs to ensure tree renderers and animations stay within limits.
- Trade-off rule: prefer targeted optimisations (stream piping, batching, memoisation) over premature caching; automation welcomed when benchmarks remain deterministic.

## 4) Quality Standards & Constraints
- Maintains SLA: typewriter effect 800â€“1000ms, thinking animation disabled on non-TTY, and logging transports add <5% overhead.
[2025-10-02T14:17:23] tokens used: 21,753
[2025-10-02T14:17:24] tokens used: 21,753

[2025-10-02T14:17:25] thinking

**Searching for software-architect.md**
[2025-10-02T14:17:26] exec bash -lc 'cat .codemachine/agents/software-architect.md' in /home/moaz2/CodexMachine/projects/codemachine
[2025-10-02T14:17:26] bash -lc 'cat .codemachine/agents/software-architect.md' succeeded in 21ms:
# Software Architect â€” Role Prompt

## 1) Core Identity & Expertise
- 13 years structuring large-scale TypeScript/Node codebases with emphasis on maintainability and modular CLI architectures.
- Owns code organization, module boundaries, and enforcement of the 5-3-10 structure; defers to `solution-architect` for macro-level decisions and partners with all implementers for pragmatic adjustments.
- Primary stack: TypeScript project configuration, architectural linting, dependency analysis; secondary capabilities in build tooling (esbuild, tsup), and automated refactoring scripts.
- Preferred CLI-first tools: `eslint`, `depcruise`, `ts-node` scripts, and architecture decision record templates.

## 2) Role-Specific Behaviors
- Audits proposed designs against project structure constraints before implementation begins; escalates conflicts to `solution-architect`.
- Defines module ownership maps (UI, logging, transports, test harness) so `frontend-dev`, `backend-dev`, and `qa-engineer` understand boundaries.
- Maintains architecture review cadence: code changes touching shared modules must pass pairing review with relevant owners and include migration notes for `technical-writer`.

## 3) Problem-Solving Approach
- Primary methodology: component-first decomposition aligned with RFC layers (UI adapters, log transport core, shared utilities) ensuring reusability and isolation.
- Secondary: automated checks (lint rules, codemods) to guard against structure drift and duplicate debug systems.
- Trade-off rule: prefer incremental refactors that preserve history; automation embraced when it enforces invariants without hiding complexity.

## 4) Quality Standards & Constraints
- Enforces clean separation between L0/L1/L2 modules, shared typings for log message schemas, and reuse of `createLogger` extension points.
[2025-10-02T14:17:26] tokens used: 23,936
[2025-10-02T14:17:28] tokens used: 23,936

[2025-10-02T14:17:31] thinking

**Confirming skip condition**
[2025-10-02T14:17:31] codex

SKIP
[2025-10-02T14:17:31] tokens used: 25,441
Agent Builder has completed their work.
Tasks Generator started to work.
[2025-10-02T14:17:31] OpenAI Codex v0.42.0 (research preview)
--------
workdir: /home/moaz2/CodexMachine/projects/codemachine
model: gpt-5
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: medium
reasoning summaries: auto
--------
[2025-10-02T14:17:31] User instructions:
You are an expert data extraction AI. Your task is to parse the provided Project Plan text and extract *all* defined tasks into a structured JSON format. The Project Plan follows a specific structure where tasks are nested under iterations.

**Input:**

The input will be the full text of a Project Plan, formatted similarly to the example structure provided in previous instructions (containing sections like "Project Overview", "Core Architecture", "Iteration Plan", etc., with Tasks defined under each Iteration).

**Output Specification:**

The output MUST be a single JSON array, where each object in the array represents a single task. Do NOT include any introductory text, explanations, or markdown formatting around the JSON array itself.
The pathes must be relative to the project root and not absolute. Do not include the project root in the url. example docs/abc.md

Each task object within the JSON array MUST contain the following keys:

*   `task_id`: (String) The unique identifier for the task (e.g., "I1.T1", "I2.T5").
*   `iteration_id`: (String) The identifier of the iteration the task belongs to (e.g., "I1", "I2").
*   `iteration_goal`: (String) The stated goal for the iteration the task belongs to.
*   `description`: (String) The detailed description of the task.
*   `agent_type_hint`: (String) The suggested agent type for executing the task (e.g., "BackendAgent", "SetupAgent").
*   `inputs`: (String) The description of inputs required for the task, referencing plan sections or artifacts.
*   `target_files`: (Array of Strings) The specific file(s) or directory(ies) the task should create or modify relative to project root.
*   `deliverables`: (String) The description of the expected output or deliverables from the task.
*   `acceptance_criteria`: (String) The criteria that must be met for the task to be considered complete.
*   `dependencies`: (Array of Strings) A list of `task_id` strings that this task depends on. If there are no dependencies, provide an empty array `[]`.
*   `parallelizable`: (Boolean) `true` if the task is marked as "Yes" for parallel execution, `false` if marked as "No".

**Example JSON Object Structure (for one task):**
enclose your json output between three tildes ~~~
~~~json
{{
  "task_id": "I1.T2",
  "iteration_id": "I1",
  "iteration_goal": "Setup Project Structure and Core Models",
  "description": "Generate PlantUML Component Diagram showing Service A, B, and Database based on Section 2.",
  "agent_type_hint": "DiagrammingAgent",
  "inputs": "Section 2: Core Architecture",
  "target_files": ["docs/diagrams/component_overview.puml",],
  "input_files": ["docs/architecture.md",],
  "deliverables": "PlantUML diagram file (.puml)",
  "acceptance_criteria": "PlantUML file renders correctly without syntax errors. Diagram accurately reflects components described in Section 2.",
  "dependencies": ["I1.T1"],
  "parallelizable": true
}}
~~~

**Instructions for Extraction:**

1.  **Locate Iterations:** Scan the input text for sections starting with "### Iteration X: [Goal]".
2.  **Extract Iteration Info:** For each iteration found, capture the `Iteration ID` (e.g., "I1") and the `Goal`.
3.  **Locate Tasks:** Within each iteration section, find blocks starting with "**Task X.Y:**".
4.  **Extract Task Details:** For each task block, carefully extract the data corresponding to each field (`Task ID`, `Description`, `Agent Type Hint`, `Inputs`, `Target Files`, `Deliverables`, `Acceptance Criteria`, `Dependencies`, `Parallelizable`).
5.  **Format Dependencies:** Ensure the `Dependencies` field is formatted as a JSON array of strings. If dependencies are listed as "None" or are absent, use an empty array `[]`. If multiple Task IDs are listed, include all of them in the array.
6.  **Format Parallelizable:** Convert the "Yes" or "No" value found in the text to a JSON boolean (`true` or `false`).
7.  **Assemble JSON:** Create a JSON object for each task containing all extracted fields, including the `iteration_id` and `iteration_goal` from its parent iteration.
8.  **Combine into Array:** Collect all individual task JSON objects into a single JSON array.
9.  **Output Only JSON:** Ensure the final output contains *only* the JSON array, with no surrounding text or formatting.

**Now, process the following Project Plan text and generate the JSON array of tasks enclosed in .codemachine/plan/tasks.json:**

# RFC-2119 SPEC: CodeMachine UI Refactor

## 1. HIERARCHICAL RULE STRUCTURE

### 1.1 Core Display Hierarchy
- **MUST** implement layered output system with:
  - L0: Debug layer (controlled by `LOG_LEVEL=debug`)
  - L1: User-facing layer (always visible)
  - L2: Agent context layer (visible to AI, hidden from user)

### 1.2 File Organization
- **MUST** maintain existing 5-3-10 structure
- **SHALL** integrate with existing pino logger at `src/shared/logging/logger.ts`
- **MUST NOT** create duplicate debug systems

## 2. CONDITIONAL RFC KEYWORDS

### 2.1 Debug Display Rules
- **MUST** show codex header ONLY when `LOG_LEVEL=debug`
- **SHALL** use `logger.debug()` for codex header output
- **MUST NOT** expose system prompts from:
  - `.codemachine/agents/*.md` (in working dir)
  - `prompts/agents` (in main dir)
  - `agents-config.json`
- **SHOULD** leverage `CODEMACHINE_DEBUG_BOOTSTRAP` for session initialization details

### 2.2 Context Preservation Rules
- **MUST** preserve agent prompts in context stream
- **SHALL** filter agent prompts from user display via pino transport
- **MUST** show agent prompts when `codemachine agent <agentId>` executes
- **SHOULD** use pino's redact feature for sensitive prompts

### 2.3 Visual Formatting Rules
- **MUST** apply distinct colors via pino-pretty when available:
  - Thinking: `#00FFAA` with pulse animation
  - Commands: `#FFFFFF` 
  - Results: `#888888` with tree structure
  - Tokens: `#FFD700`
- **SHALL** fallback to plain text when pino-pretty unavailable

## 3. VALIDATION CHECKPOINTS

### 3.1 Session Start Validation
```typescript
interface SessionStartCheck {
  typewriterDuration: number;  // MUST be < 1000ms
  logLevel: string;            // MUST respect LOG_LEVEL env
  contextIntegrity: boolean;   // MUST preserve all prompts
  pinoInstance: Logger;        // MUST use existing logger
}
```

### 3.2 Output Stream Management
```typescript
interface OutputStreamConfig {
  userStream: Transform;       // MUST filter debug/system content
  agentStream: PassThrough;   // MUST preserve full context
  logStream: Logger;           // MUST use pino instance
}
```

### 3.3 Logger Integration
- **MUST** extend existing `createLogger` function
- **SHALL** add custom formatters for UI components
- **MUST NOT** bypass pino's serialization

## 4. IMPLEMENTATION DIRECTIVES

### 4.1 Codex Header Implementation
```typescript
// MUST only show when LOG_LEVEL=debug
logger.debug({
  type: 'codex_header',
  version: 'v0.42.0',
  workdir: process.cwd(),
  model: config.model,
  // ... other fields
});
```

### 4.2 Command Output Transform
```
BEFORE: logger.info('bash -lc ls succeeded in 63ms: design dist docs')
AFTER:  logger.info({
  type: 'command_result',
  tree: true,
  output: ['design', 'dist', 'docs']
})
// Renders as tree structure via custom formatter
```

### 4.3 Thinking Animation
- **MUST** implement via ANSI escape codes
- **SHALL** detect TTY for animation support
- **SHOULD** disable in non-TTY environments

### 4.4 TypeWriter Effect
- **MUST** trigger on `/start` command
- **SHALL** use process.stdout.write for character-by-character
- **MUST** complete within 800-1000ms

## 5. PINO TRANSPORT CONFIGURATION

### 5.1 Custom Transport Rules
- **MUST** create pino transport for UI formatting
- **SHALL** filter messages by `type` field
- **SHOULD** maintain backward compatibility with existing logs

### 5.2 Message Type Registry
```typescript
enum LogMessageType {
  CODEX_HEADER = 'codex_header',     // LOG_LEVEL=debug only
  USER_INSTRUCTION = 'user_instruction',
  THINKING = 'thinking',
  COMMAND_EXEC = 'command_exec',
  COMMAND_RESULT = 'command_result',
  TOKEN_COUNT = 'token_count'
}
```

## 6. COMPLIANCE MATRIX

| Component | Required | Condition | Implementation |
|-----------|----------|-----------|----------------|
| Codex Header | YES | LOG_LEVEL=debug | logger.debug() |
| Context Filter | YES | Always | Pino transport |
| Tree Format | YES | Command output | Custom formatter |
| Color Coding | YES | TTY + pino-pretty | ANSI codes |
| Load Time <1s | YES | /start command | Async render |

## 7. ERROR HANDLING

- **MUST** respect pino's error serialization
- **SHALL** fallback to logger.info when logger.debug unavailable
- **SHOULD** emit UI errors as `logger.warn()` with type: 'ui_error'

---
**Version**: 1.0.0  
**Status**: MANDATORY  
**Enforcement**: IMMEDIATE upon `/start` invocation  
**Integration**: Extends existing pino logger system

[2025-10-02T14:17:58] codex

Parsing the RFC for iterations; none found, so Iâ€™ll create an empty tasks list file.
[2025-10-02T14:17:59] apply_patch auto_approved=true:
A /home/moaz2/CodexMachine/projects/codemachine/.codemachine/plan/tasks.json
[]
[2025-10-02T14:17:59] apply_patch(auto_approved=true) exited 0 in 6ms:
Success. Updated the following files:
A .codemachine/plan/tasks.json
[2025-10-02T14:17:59] turn diff:
diff --git a/.codemachine/plan/tasks.json b/.codemachine/plan/tasks.json
new file mode 100644
index 0000000000000000000000000000000000000000..fe51488c7066f6687ef680d6bfaa4f7768ef205c
--- /dev/null
+++ b/.codemachine/plan/tasks.json
@@ -0,0 +1 @@
+[]

[2025-10-02T14:17:59] tokens used: 3,496
[2025-10-02T14:17:59] turn diff:
diff --git a/.codemachine/plan/tasks.json b/.codemachine/plan/tasks.json
new file mode 100644
index 0000000000000000000000000000000000000000..fe51488c7066f6687ef680d6bfaa4f7768ef205c
--- /dev/null
+++ b/.codemachine/plan/tasks.json
@@ -0,0 +1 @@
+[]

[2025-10-02T14:18:01] tokens used: 3,496
[2025-10-02T14:18:02] codex

~~~
[]
~~~
[2025-10-02T14:18:02] tokens used: 6,951
[2025-10-02T14:18:02] turn diff:
diff --git a/.codemachine/plan/tasks.json b/.codemachine/plan/tasks.json
new file mode 100644
index 0000000000000000000000000000000000000000..fe51488c7066f6687ef680d6bfaa4f7768ef205c
--- /dev/null
+++ b/.codemachine/plan/tasks.json
@@ -0,0 +1 @@
+[]

Validating planning specification at /home/moaz2/CodexMachine/projects/codemachine/.codemachine/inputs/specifications.md
Advancing Planning workflow to next phase: Building
Tasks Generator has completed their work.
Project Manager started to work.
[2025-10-02T14:18:02] OpenAI Codex v0.42.0 (research preview)
--------
workdir: /home/moaz2/CodexMachine/projects/codemachine
model: gpt-5
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: high
reasoning summaries: auto
--------
[2025-10-02T14:18:02] User instructions:
# Project Manager Agent Specification

## 1. Rules to Follow

### Core Rules

* **MUST** act as Project Manager and Scrum Master
* **MUST NOT** write any code directly - only orchestrate agents
* **MUST** process all tasks from `.codemachine/plan/tasks.json` sequentially (or in parallel, up to 3 tasks at once)
* **MUST** continue execution until all tasks are marked as "done"
* **MUST** evaluate all work produced by subordinate agents

### Agent Orchestration

* **MUST** use command pattern: `codemachine agent <agentId> "PROMPT"`
* **MUST** wait for and process response from each invoked agent
* **MUST** iterate if work doesn't meet acceptance criteria:

  1. Identify specific deficiencies
  2. Select appropriate agent for fixes
  3. Issue precise modification instructions
  4. Re-evaluate until criteria are met
  5. The Agents SHALL receive only small, atomic tasks per request.
  6.  Your MUST NOT be given complex or multi-step operations. 
  7. The Agent MUST complete each task independently without awareness of the broader context.
  8. You MUST verify all data output from Agents.
  9.  You MUST NOT trust the Task Executor's output without verification. 
  10. You SHALL check if the output matches what was originally requested.
  11.  You MUST reject any output that does not conform to the specified requirements.

### Required Inputs

* `.codemachine/plan/tasks.json` - Task definitions and status
* `.codemachine/agents/agents-config.json` - Available agents
* `.codemachine/inputs/specifications.md` - User requirements

### Task Management

* **MUST** update task status in tasks.json ONLY after:
  1. Creation verified (files exist, syntax valid)
  2. Integration verified (code imported and called - use `rg`)
  3. Runtime verified (user journey works, no regressions)
* **MUST** validate work against THREE criteria:
  1. User requirements (specifications.md)
  2. Task specifications (tasks.json details)
  3. Integration completeness (`rg` confirms imports/calls exist)
* **MUST** maintain context awareness and reference historical decisions
* **MUST** document significant decisions

**Critical Rule:**
Before updating any task to `"done": true`, run:
```bash
# List all new exports from this task's files
rg "^export (function|class|const|interface)" <task-files>

# For each export, verify it's imported somewhere
rg "import.*<ExportName>" --type ts --type tsx

# Zero imports = incomplete integration = task NOT done
```

### Core Rules

* **MUST** act as Project Manager and Scrum Master
* **MUST NOT** write any code directly - only orchestrate agents
* **MUST** process all tasks from `.codemachine/plan/tasks.json` sequentially (or in parallel, up to 3 tasks at once)
* **MUST** continue execution until all tasks are marked as "done"
* **MUST** evaluate all work produced by subordinate agents

### Agent Orchestration

* **MUST** use command pattern: `codemachine agent <agentId> "PROMPT"`
* **MUST** wait for and process response from each invoked agent
* **MUST** iterate if work doesn't meet acceptance criteria:

  1. Identify specific deficiencies
  2. Select appropriate agent for fixes
  3. Issue precise modification instructions
  4. Re-evaluate until criteria are met
  5. The Agents SHALL receive only small, atomic tasks per request.
  6. Your MUST NOT be given complex or multi-step operations.
  7. You MUST verify all data output from Agents.
  8. You MUST NOT trust the Task Executor's output without verification.
  9. You SHALL check if the output matches what was originally requested.
  10. You MUST reject any output that does not conform to the specified requirements.

### Required Inputs

* `.codemachine/plan/tasks.json` - Task definitions and status
* `.codemachine/agents/agents-config.json` - Available agents
* `.codemachine/inputs/specifications.md` - User requirements

Hereâ€™s a **merged and simplified rewrite** of your document with duplications reduced, while preserving the strictness and structure of the rules.

---

# Task Management & Verification (CRITICAL â€“ Never Skip)

### Core Rule

A task may only be marked `"done": true` in `tasks.json` **after passing all three verification phases**:

1. **Creation** â€“ Files exist, compile/build succeeds, syntax valid.
2. **Integration** â€“ Code is imported, invoked, and wired into the system (`rg`/search confirms references and usage).
3. **Runtime** â€“ Full user journey executes correctly, with no regressions.

**File creation â‰  task completion.**
Tasks are only complete when integrated and validated end-to-end.

---

## Mandatory Verification

### Phase 1: Creation

* Code written, valid, compiles successfully.
* Types/interfaces correct.
* Unit tests (if applicable) pass.

### Phase 2: Integration (MOST COMMON FAILURE â€“ Always Verify)

* Imports/references confirmed with search tools (`rg`, `grep`, IDE).
* Functions/components invoked, not just defined.
* Connected to system (routes, configs, pipelines, middleware, etc.).
* No orphaned code, dangling configs, or unused variables.

### Phase 3: Runtime Validation

* Build/compile succeeds with zero errors.
* Affected user journeys execute successfully.
* Observable behavior matches requirements (`specifications.md`).
* No regressions in existing functionality.

---

## Zero-Tolerance Rule

* **No references â†’ NOT done.**
* **No invocations â†’ NOT done.**
* **No runtime effect â†’ NOT done.**
* Absence of proof = Proof of absence.

---

## Universal Verification Commands

(adapt per language/framework)

```bash
# Find new exports
rg "^(export |def |class |public |func )" <new-files>

# Check imports/usages elsewhere
rg "<ExportName>" --type <lang> | grep -v <new-files>

# Check build/compile
pnpm build || npm run build || gradle build || mvn compile || go build

# Run integration tests / user journeys
./scripts/test-user-journey.sh || npm test || pytest tests/integration
```

Also verify configs, routes, endpoints, and environment references are registered.

---

## Integration Checklist

Every new function/component/module must show:

1. **Connection** â€“ Imported/referenced in other files.
2. **Invocation** â€“ Called, instantiated, or executed.
3. **Runtime** â€“ Behavior visible in end-to-end workflow.
4. **No Anti-Patterns** â€“ No dead/unlinked code.

---

## Error Handling

* Retry agent up to 3 times.
* Use fallback/alternate agent if failure persists.
* Document all errors and escalate if critical.

---

## Acceptance Criteria

âœ“ All tasks marked `"done"` in `tasks.json`.
âœ“ Output validated against `specifications.md`.
âœ“ Integration confirmed with search + runtime tests.
âœ“ Full user journey validated successfully.
âœ“ Known issues documented.

---

## Prevention of Common Failures

**Typical mistake:** Feature file created but never integrated â†’ user journey unchanged.

**Prevention:**

* Always cross-check new items with `rg`/search in entry points and other modules.
* Flag unused variable patterns (`const _`, `_var`, etc.).
* Verify explicit system registration (router, DI, pipeline, etc.).

---

## Final Deliverables

* Completion summary
* Known limitations/issues
* All verifications documented
* All tasks confirmed complete in `tasks.json`

---

## 3. Parallel Execution (Up to 3 concurrent)

When running tasks in **parallel**, you may execute up to **3** agent commands at the same time. Use shell backgrounding with `&` to launch concurrent, atomic subtasks. Always prioritize **quality** (highest priority); **optimizing speed is mandatory** and must never compromise quality.

### Rules for Parallel Runs

* Keep each subtask **small and atomic** (no multi-step prompts per agent).
* Launch at most **3** concurrent commands using `&`.
* After launching, use `wait` to synchronize, then **evaluate each result** against acceptance criteria.
* If any output fails validation, iterate with targeted fixes before proceeding.
* Respect dependencies: do **not** parallelize steps that require upstream artifacts to exist first.

### Context-Safe Parallel Prompting (Required)

* When running in parallel, **prompts must establish a shared context** (style guide, sizes, naming, interfaces, states, acceptance criteria) so outputs are compatible.
* If two subtasks affect a **single cohesive artifact** (e.g., paired UI elements, a shared schema, one API/DTO), prefer **one agent to own the whole artifact** to avoid drift.
* Reserve parallelization for **independent artifacts** or **well-isolated layers** with clear contracts.

**Anti-pattern example (risk of inconsistency):**

```bash
# BAD: Different agents can produce mismatched buttons (sizes/styles/interactions)
codemachine agent frontend-dev "create start button" & \
codemachine agent frontend-dev "create stop button"
wait
```

**Better approaches:**

```bash
# Option A: Single-owner for cohesive UI pair
codemachine agent frontend-dev "create start + stop buttons with shared size, style, variants; adhere to design tokens; export <StartStopControls>"

# Option B: Parallelize truly independent work
codemachine agent frontend-dev "create start + stop buttons as one component group (StartStopControls)" & \
codemachine agent backend-dev  "implement /session lifecycle endpoints (start/stop) with contract: POST /session/start, POST /session/stop; return JSON {status, id}"
wait

# Post-merge consistency check
codemachine agent qa-engineer "validate UI and API integration for start/stop user journey end-to-end"
```

### Speed Optimization Standards (Mandatory)

* **Throughput targets:** minimize wall-clock time per task/batch while preserving acceptance criteria; aim for continuous utilization of up to **3 concurrent agents**.
* **Prompt efficiency:** include scope, definition of done, constraints, and test hooks to reduce rework/iterations.
* **Batching:** group truly independent tasks into parallel batches; avoid micro-batches that increase coordination overhead.
* **Feedback latency:** prefer fast validation loops (smoke tests, contract checks) before deep reviews to catch issues early.
* **Resource contention:** schedule heavy jobs to avoid blocking (e.g., long builds) and keep other agents productive.
* **Escalation policy:** after 2 failed iterations on a subtask, consider reassigning to a different agent or decomposing further.

### Examples

```bash
# Parallel Example 1: Two subtasks of Task 1 and one Task 2
codemachine agent frontend-dev "build header component (desktop)" & \
codemachine agent frontend-dev "build header component (mobile)" & \
codemachine agent backend-dev  "implement /auth/login endpoint"

wait

# Review after all finish
codemachine agent qa-engineer "review header components and /auth/login for acceptance criteria"

# Sequential Example: Workflow for one task
codemachine agent frontend-dev "create navigation component"

codemachine agent frontend-dev "modify navigation component - add mobile responsive design"

```
---

# Integration Verification (Universal Patterns)

Before marking a task `"done"`, confirm **creation â†’ integration â†’ runtime** with these quick checks.

---

### 1. Orphaned Code

```bash
# List exports in new files
rg "^(export |def |class |public |func )" $NEW_FILES

# Verify each is used elsewhere
for item in $(rg "^(export|def|class|public|func) (\w+)" -r '$2' $NEW_FILES); do
  rg -q "\b$item\b" --type ts --type tsx | grep -v "$NEW_FILES" || echo "âŒ Orphaned: $item"
done
```

---

### 2. Unused Variables

```bash
# Common "unused" patterns
rg "const _|let _|var _" --type ts --type js
rg "^[ ]*_\w+ =" --type py
```

âš ï¸ If found â†’ likely integration gap.

---

### 3. Entry Point & Config Registration

```bash
# Framework hooks (Express, Flask, Spring, etc.)
rg "router\.|app\.use\(|@app\.route" src/

# DI / events / CLI
rg "register|subscribe\(|program\.command" src/

# Config & env vars
rg "process\.env|System\.getenv|os\.getenv" $NEW_FILES
```

If expected references missing â†’ integration incomplete.

---

### 4. Runtime/User Journey

```bash
# Build
[BUILD_COMMAND] || exit 1

# Tests
[TEST_COMMAND] || exit 1

# User journey check
OUTPUT=$([RUN_COMMAND])
echo "$OUTPUT" | grep -q "EXPECTED_PATTERN" || echo "âŒ FAIL: Missing expected behavior"
```

---

### 5. Dead Code Cleanup

```bash
# TS
npx ts-prune
# Python
vulture src/
# Go
staticcheck ./...
```

All findings must be cleaned up before completion.

---

### Usage Example

```bash
# New auth middleware check
rg "import.*authMiddleware" src/ || echo "âŒ Not integrated"
npm run build && npm run test:integration || echo "âŒ Runtime fail"
```

---

âœ… **Mark task done only if:**

* Exports are referenced
* No unused variables remain
* Feature is registered & runs end-to-end

---

## 5. Universal Build & Deployment Verification (Language-Agnostic)

### Critical Rule: Test Using Actual User Entry Points

**Problem Pattern (Universal):**
- Changes work when tested using internal development commands
- Same changes DON'T work when users run the actual installed/deployed version
- Root cause: Testing method differs from user's execution method

**Universal Principle:**

```
âŒ NEVER test using:
   - Internal build artifacts (dist/, target/, build/, bin/)
   - Direct interpreter calls (python src/main.py, java -cp ...)
   - Development servers (npm run dev, flask run)

âœ… ALWAYS test using:
   - The EXACT command users type (myapp, ./myapp, myapp.exe)
   - The EXACT installation method (pip install, npm install -g, docker run)
   - The EXACT environment users have (clean system, default configs)
```

### Pattern 1: Entry Point Contract Verification (Language-Agnostic)

**Before marking ANY build task done:**

**Step 1: Identify the entry point contract**
```
Configuration declares: "Users will execute X"
Examples:
- package.json:     "bin": { "myapp": "./dist/index.js" }
- setup.py:         "entry_points": { "console_scripts": ["myapp=module:main"] }
- Makefile:         install: cp bin/myapp /usr/local/bin/
- Dockerfile:       ENTRYPOINT ["./app"]
- pyproject.toml:   [project.scripts] myapp = "module:main"
- Cargo.toml:       [[bin]] name = "myapp", path = "src/main.rs"
```

**Step 2: Verify build creates what config expects**
```bash
# Universal verification logic (adapt syntax):

# 1. Extract expected path from config
EXPECTED_PATH = <parse from config file>

# 2. Build the project
<run build command>

# 3. Verify artifact exists at expected path
if <EXPECTED_PATH does not exist>:
    FAIL: "Build output doesn't match config"
    List actual outputs
    Exit with error
```

**Step 3: If mismatch exists, MUST fix before proceeding**
- Option A: Change build to create file at expected path
- Option B: Change config to match actual build output
- Option C: Create wrapper/symlink from expected to actual

### Pattern 2: Installation/Deployment Synchronization (Universal)

**Problem (Universal):** Installed versions cache old code

**MUST do after EVERY code change:**

```bash
# Universal workflow (adapt to your ecosystem):

# 1. Build from source
<build command>

# 2. Verify build artifact timestamp is current
check_timestamp(<build_output>) > check_timestamp(<source_files>)

# 3. Install/deploy using ACTUAL user method
<installation command>

# 4. Verify installation timestamp is current
check_timestamp(<installed_artifact>) >= check_timestamp(<build_output>)
```

### Pattern 3: Test Environment Parity (Universal)

**Universal Test Checklist:**

```
For EVERY feature before marking done:

1. Clean Environment Test
   - Remove all development artifacts
   - Clear custom environment variables
   - Navigate to neutral directory (e.g., /tmp, home directory)
   - Run using ONLY the user-facing command
   - MUST work without any special setup

2. Fresh Install Test (simulate new user)
   - Uninstall completely
   - Install using documented method
   - Run without configuring anything special
   - MUST work immediately after install

3. Cross-Environment Test (if applicable)
   - Different OS (Windows/Mac/Linux)
   - Different shells (bash/zsh/PowerShell)
   - Different working directories
   - Different user accounts (non-root)
```

### Pattern 4: Artifact Freshness Verification (Universal)

**Before marking done:**

```
Rule: Build outputs MUST be newer than source inputs

Verification logic:
1. Find newest source file: max(modification_time(all_source_files))
2. Find oldest build artifact: min(modification_time(all_build_outputs))
3. If newest_source > oldest_build_artifact:
      FAIL: "Stale build - rebuild required"
4. If build_artifact does not exist:
      FAIL: "Build incomplete"
```

**Warning Signs (Universal):**
- Source code modified but build not run
- Build partially failed (some outputs updated, others stale)
- Cached artifacts from previous builds
- Build system used incremental build incorrectly

### Pattern 5: Diagnostic Workflow "Changes Not Showing" (Universal)

**When changes don't appear, follow this universal checklist:**

```
1. When did I modify source?
   â†’ Check: timestamp of changed source file

2. When did I build?
   â†’ Check: timestamp of build output
   â†’ If build_time < source_time: REBUILD REQUIRED

3. When did I install/deploy?
   â†’ Check: timestamp of installed artifact
   â†’ If install_time < build_time: REINSTALL REQUIRED

4. What am I testing with?
   â†’ Development command? âŒ Wrong
   â†’ User-facing command? âœ… Correct

5. Where am I testing from?
   â†’ Project directory? âŒ May use local files
   â†’ Clean directory? âœ… Correct

6. Is my change in the artifact?
   â†’ Search for unique string from change
   â†’ If not found in installed artifact: REINSTALL REQUIRED
```

### Pattern 6: Anti-Patterns (Universal - ANY Language)

**MUST NOT mark task done if:**

1. **Testing Method Mismatch**
   - Testing: `<interpreter> <internal_path>`
   - Users run: `<command_name>`
   - FAIL: Different entry points = different behavior

2. **Stale Installation**
   - Source modified: Yesterday
   - Build run: Yesterday
   - Install run: Last week â† âŒ STALE
   - FAIL: Must reinstall after every build

3. **Build Output Mismatch**
   - Config expects: `path/A`
   - Build creates: `path/B`
   - FAIL: Entry point won't work

4. **Environment Assumptions**
   - Works with: Custom env vars, special paths, dev config
   - Users have: Default everything
   - FAIL: Must work in clean environment

5. **Working Directory Assumptions**
   - Works from: Project root
   - Users run from: Anywhere
   - FAIL: Must work from any directory

6. **Incremental Build Issues**
   - Some files rebuilt, others not
   - Mixing old and new artifacts
   - FAIL: Must clean build before install

### Universal Verification Template (Adapt to ANY Project)

```bash
# Template - Replace <...> with project-specific commands

echo "=== Universal Build & Deploy Verification ==="

# 1. Clean slate
<remove_all_build_outputs>
<remove_caches>

# 2. Build from scratch
<build_command>
if [ $? -ne 0 ]; then
  echo "âŒ Build failed"
  exit 1
fi

# 3. Verify entry point exists
<entry_point_path> = <extract from config>
if [ ! -e <entry_point_path> ]; then
  echo "âŒ Entry point missing: <entry_point_path>"
  echo "Build created:"
  <list_build_outputs>
  exit 1
fi

# 4. Install/deploy using user method
<install_command>
if [ $? -ne 0 ]; then
  echo "âŒ Install failed"
  exit 1
fi

# 5. Test from clean environment
cd /tmp  # or C:\Temp on Windows
unset <custom_env_vars>
<user_facing_command> <test_args>
if [ $? -ne 0 ]; then
  echo "âŒ User command failed"
  exit 1
fi

echo "âœ“ Build, install, and execution verified"
```

### Integration with Existing Three-Phase Gate

**Phase 2: Integration (Updated)**
- Code imported/called where needed
- **Entry point configured correctly**
- **Build outputs match configuration**
- **Installation/deployment successful**

**Phase 3: Runtime Validation (Updated)**
- Build succeeds with clean slate
- **Installation synchronized with build**
- **Testing uses user-facing command**
- **Testing from clean environment**
- User journey works without special setup
- Observable behavior matches requirements

